CSV Header:
DATE,TIME,BOROUGH,ZIP CODE,LATITUDE,LONGITUDE,LOCATION,ON STREET NAME,CROSS STREET NAME,OFF STREET NAME,NUMBER OF PERSONS INJURED,NUMBER OF PERSONS KILLED,NUMBER OF PEDESTRIANS INJURED,NUMBER OF PEDESTRIANS KILLED,NUMBER OF CYCLIST INJURED,NUMBER OF CYCLIST KILLED,NUMBER OF MOTORIST INJURED,NUMBER OF MOTORIST KILLED,CONTRIBUTING FACTOR VEHICLE 1,CONTRIBUTING FACTOR VEHICLE 2,CONTRIBUTING FACTOR VEHICLE 3,CONTRIBUTING FACTOR VEHICLE 4,CONTRIBUTING FACTOR VEHICLE 5,UNIQUE KEY,VEHICLE TYPE CODE 1,VEHICLE TYPE CODE 2,VEHICLE TYPE CODE 3,VEHICLE TYPE CODE 4,VEHICLE TYPE CODE 5

IMPORTANTE: 
  - Quando sono utilizzate le mappe ricordarsi che l'operatore [] se l'elemento non è presente va sempre ad inserire un nuovo elemento con la chiave designata e con rispettivo valore inizializzato (es. se è un intero viene messo a 0)
  - The arrays allocated in this way aren't linear in memory, and cannot be used by the MPI functions globally. When you pass C, you are passing just the array of pointers. Passing &C[0][0], you are passing the array of the first 3 elements, but the other 3-elements-arrays aren't contiguous in memory, since they've been allocated independently. So a segfault is the best that you can achieve, being random results, the worst.

Todo:
  
  ✔ Capire perchè non CSVRow.cpp non vede le reference a CSVRow.h con code runner @started(20-02-26 23:35) @done(20-03-01 13:07) @lasted(3d13h32m53s)
    Per il momento inseriamo tutto nell'header file a fini di testing. Probabilmente è un problema non risolvibile se non creando uno script. Essendo un lavoro un po' troppo tedioso credo che lascerò per il momento tutto così e in un secondo momento, quando il progetto sarà più robusto, creerò un makefile.

  ✔ Check data and processing correctness @started(20-02-27 15:24) @done(20-03-01 13:07) @lasted(2d21h43m35s)
    Ogni tupla del dataset ha una unique key che è possibile sfruttare per ricercare velocemente una determinata righa all'interno del file.
    
    ✔ Number of lethal accidents per week throughout the entire dataset. @started(20-02-27 15:27) @done(20-02-28 15:57) @lasted(1d30m45s)
      In un anno ci sono 52 weeks. Per ogni collisione vado a vedere se ci sono stati dei morti, in caso affermativo vado ad incrementare il numero di lethal accidents.
      Conviene creare tante mappe quanti sono gli anni all'interno del dataset, quindi dato che va dal 2012 al 2017 si avranno 6 mappe.
      In ognuna di esse si utilizzerà una mappa del tipo {weekNumber, lethalAccidents}.

    ✔ Number of accidents and percentage of number of deaths per contributing factor in the dataset.@started(20-02-28 01:31) @done(20-03-01 13:07) @lasted(2d11h36m22s)
      I.e., for each contributing factor, we want to know how many accidents were due to that contributing factor and what percentage of these accidents were also lethal.
      
      Il contributing factor può anche essere 'Unspecified' o 'Empty', in tal caso sarà ignorato durante il parsing.
      Ovviamente se una collisione ha uno o più contributing factor uguali lo andiamo a considerare solo una volta. Quindi guardiamo il CF_1, poi il CF_2, ecc.
      -> In ogni caso non c'è CF_5 senza CF_4, CF_4 senza CF_3 ecc. Quindi si può sempre partire da CF_1 e scorrere avanti.

      [numero di incidenti] basta semplicemente contare il numero di tuple all'interno del dataset corrispondenti a quel particolare contributing factor. 
      [percentuale di incidenti letali] durante il ciclo si aumenta un counter corrispondente al numero di incidenti letali e alla fine per calcolare la percentuale si farà -> perc = num_inc_letali / num_inc

      Sempre seguendo l'idea delle mappe conviene fare una mappa per ogni contributing factor ed associarvi una coppia std::pair contenente le due informazioni.
    
    ✔ Number of accidents and average number of lethal accidents per week per borough. @done(20-03-01 13:07)
      I.e., for each borough, we want to know how many accidents there were in that borough each week, as well as the average number of lethal accidents that the borough had per week.

      Borough (quartieri) nel dataset -> [MANHATTAN, QUEENS, BRONX, BROOKLYN, STATEN ISLAND]

      Trovati tutti i possibili borough nel dataset ed associando ad ognuno di essi una mappa {week, {numAccidents, numLethalAccidents}}, scorrendo il dataset per ogni tupla ottengo il borough, la week e il numero di morti (per capire se è un incidente letale o meno). Con questi dati a disposizione vado a popolare la mia struttura d'appoggio.
      -> A volte il borough non è specificato (e.s. stringa vuota) quindi filtrare questa possibilità.

      Per stampare l'output scorro la struttura dati e per ogni borough, stampo la lista delle weeks con affianco il numero di incidenti salvandomi a parte via via la somma degli incidenti letali. A fine calcolo l'average facendo numLethalAccidents / numWeeks.
      
     ✔ Fixa il parsing delle righe nel caso in cui si abbia una stringa quotata @started(20-02-29 02:51) @done(20-02-29 10:45) @lasted(7h54m)
      E.s. "IF/O 519 GATEWAY DRIVE,BROOKLYN,NY 11239"  

      
      ✔ Provare a testare se sia possibile integrare MPI/OpenMP oppure serva qualcosa di più "raw" @started(20-02-27 18:22) @done(20-03-16 15:35) @lasted(2w3d21h13m1s)
      Leggendo online l'unica datastructure (oltre ovviamente agli array in C) che supporta MPI/OpenMP (o almeno la versione d'utilizzo) è il vector facendo però attenzione che tutte le azioni che implicano un resize (e.s. push_back()) non sono thread-safe!
      
      ✔ Valutare la creazione di una struct personalizzata con MPI dove mettere tutti i dati necessari per le tre query in fase di loading @started(20-03-12 01:37) @done(20-03-16 15:35) @lasted(4d13h58m5s)
      
      ✔ Lettura dello stesso file da più processi utilizzando MPI_IO() @started(20-03-21 19:32) @done(20-03-21 19:32) @lasted(51s)

      ☐ I buffers per le query sono tutti statici. C'è il modo di rendergli dinamici? @started(20-03-22 11:18)
      
      ☐ Separare meglio i file sia di OpenMP che di MPI @started(20-03-31 16:06)
        Vogliamo utilizzare sempre lo stesso codice ma che vada bene sia per un'esecuzione completamente seriale, sia solo sfruttando OpenMP e sia sfruttando solo MPI.
        I file che uniscono le due tecnologie saranno messi fuori dalle cartelle e trattati come "main" della nostra applicazione.

      ☐ Controllare che l'introduzione di openMP sia ottimale e non ci siano modi più smart @started(20-03-16 15:36)
        ☐ Forse sarebbe meglio dividere in 3 sezioni le query in modo tale che se un thread finisce le sue queries può passare a svolgere le query di un altro thread?
        ☐ Aggiungere delle stampe fatte da ogni processo?
          Ad es. il numero di thread che stanno eseguendo la query 1, il numero di thread che eseguono la query 2 ecc ecc.
        ☐ Aggiungere un nowait al for loop del processing?
        ☐ Aggiungere uno scheduling di tipo dinamico (in questo modo riusciamo teoricamente a velocizzare al massimo l'esecuzione)
        ☐ Non credo serva specificare la clausola default() ma controlla. 
          Già di default tutte le variabili non specificate all'interno della sezione parallel (es. le strutture dati che memorizzano i risultati delle query) sono settate shared ed è quello che vogliamo visto che l'unica rilevante è localRows, che è appunto utilizzata da tutti senza problemi di race conditions essendo solo letta.

      ☐ Aggiungere la parte di statistica. @started(20-03-16 15:37)
        Dobbiamo misurare i tempi d'inizio e fine di ogni sotto processo, in particolare della fase di loading, di processing e di stampa dei risultati.
        ✔ Valutare se convenga o meno utilizzare MPI_Wtime() a tal fine @done(20-03-18 22:39)
          Funziona perfettamente anche usando semplicemente cpu_seconds().

        ✔ Parametrizzare il numero di OpenMP threads e il numero di MPI processes  @started(20-03-18 22:42) @done(20-03-19 11:58) @lasted(13h16m1s)
          Vogliamo tunare questi parametri per trovare quelli che ci forniscono il tempo d'esecuzione migliore. Più semplicemente vogliamo fare un'analisi di scalabilità andando ad aumentare il numero di nodi a seconda dei valori assunti da questi paramentri.
          ☐ Introducendo i due parametri direttamente dal terminale (quindi user input) è l'opzione migliore.
            ./allQueries_mpi_omp.o -np <num_mpi_processes> <num_omp_threads>
      
      ☐ Modulare il codice creando funzioni d'appoggio o anche nuove classi per avere un codice più pulito e poter distribuire meglio i vari sotto processi @started(20-03-12 11:33)

      ✔ Verificare se sia possibile spostare i processi su più macchine (MPI) @started(20-03-16 15:41) @done(20-03-31 12:29) @lasted(2w19h48m8s)
      
      
Minors:
      ☐ Aggiungere al codice dell'error handling (es MPI_SUCCESS returns etc.) @started(20-03-12 01:40)
  
      ☐ Trovare una buona libreria per plottare il grafico dei tempi al variare del numero di processi e dei threads. @started(20-02-27 18:21)
      
Additional informations:
  -  https://stackoverflow.com/questions/10766263/message-passing-interface-on-shared-memory-systems-performance
  Let's assume we only consider MPI and OpenMP, since they are the two major representatives of the two parallel programming families you mention. For distributed systems, MPI is the only option between different nodes. Within a single node, however, as you well say, you can still use MPI and use OpenMP too. Which one will perform better really depends on the application you are running, and specifically in its computation/communication ratio. Here you can see a comparison of MPI and OpenMP for a multicore processor, where they confirm the same observation.
  You can go a step further and use a hybrid approach. Use MPI between the nodes and then use OpenMP within nodes. This is called hybrid MPI+OpenMP parallel programming. You can also apply this within a node that contains a hybrid CMP+SMT processor.
  You can check some information here and here. Moreover this paper compares an MPI approach vs a hybrid MPI+OpenMP one.

  - Contributing Factors
  contributingFactors = {
    {"Accelerator Defective", 0},
    {"Aggressive Driving/Road Rage", 1},
    {"Alcohol Involvement", 2},
    {"Animals Action", 3},
    {"Backing Unsafely", 4},
    {"Brakes Defective", 5},
    {"Cell Phone (hand-held)", 6},
    {"Cell Phone (hands-free)", 7},
    {"Driver Inattention/Distraction", 8},
    {"Driver Inexperience", 9},
    {"Driverless/Runaway Vehicle", 10},
    {"Drugs (Illegal)", 11},
    {"Failure to Keep Right", 12},
    {"Failure to Yield Right-of-Way", 13},
    {"Fatigued/Drowsy", 14},
    {"Fell Asleep", 15},
    {"Following Too Closely", 16},
    {"Glare", 17},
    {"Headlights Defective", 18},
    {"Illness", 19},
    {"Lane Marking Improper/Inadequate", 20},
    {"Lost Consciousness", 21},
    {"Obstruction/Debris", 22},
    {"Other Electronic Device", 23},
    {"Other Lighting Defects", 24},
    {"Other Vehicular", 25},
    {"Outside Car Distraction", 26},
    {"Oversized Vehicle", 27},
    {"Passenger Distraction", 28},
    {"Passing or Lane Usage Improper", 29},
    {"Pavement Defective", 30},
    {"Pavement Slippery", 31},
    {"Pedestrian/Bicyclist/Other Pedestrian Error/Confusion", 32},
    {"Physical Disability", 33},
    {"Prescription Medication", 34},
    {"Reaction to Other Uninvolved Vehicle", 35},
    {"Shoulders Defective/Improper", 36},
    {"Steering Failure", 37},
    {"Tire Failure/Inadequate", 38},
    {"Tow Hitch Defective", 39},
    {"Traffic Control Device Improper/Non-Working", 40},
    {"Traffic Control Disregarded", 41},
    {"Turning Improperly", 42},
    {"Unsafe Lane Changing", 43},
    {"Unsafe Speed", 44},
    {"View Obstructed/Limited", 45},
    {"Windshield Inadequate", 46}
  }

  - Boroughs
  boroughs{
    {"BRONX", 0},
    {"BROOKLYN", 1},
    {"MANHATTAN", 2},
    {"QUEENS", 3},
    {"STATEN ISLAND", 4}
  }

OpenMP Notes:

      #pragma omp directive_name [clause, ...]
      {
      …
      }
    
    - I/O
      – OpenMP specifies nothing about parallel I/O
      – The programmer has to insure that I/O is conducted correctly within the context of a multi-threaded program
      
    - Compilation
      g++ hello.cc -fopenmp -o hello
      
    - Runtime 
      • If any thread terminates within a parallel region, all
      threads in the team terminate...
      • ...and the work done up until that point is undefined
      
    - API
      void omp_set_num_threads(int num_threads)
      
      int omp_get_num_threads(void)
    
      int omp_get_thread_num(void)
    
    - [Directives] parallel
      #pragma omp parallel [clause, clause, ... ]
      {
      // block
      }
      • When a thread reaches a parallel directive, it creates a team of threads and becomes the master of the team
      • The master is a member of that team and has thread number 0 within that team
        – Other threads have number 1...N-1
    
    - [Directives] for
        #pragma omp for [clause ...]
        for_loop
    
        This must be ecnlosed inside a parallel region in order for the directive to execute in parallel ( #pragma omp parallel for [clause ...] ). 
        This directive shares iterations of a for loop (which is indeed needed by the syntax). 
        - [Clauses] schedule
            schedule (type [,chunk])
            Describe how iterations of the loop are divided among the threads in the team. Default schedule is implementation dependent. 
                -> type
                    static : loop iterations are divided into blocks of size chunk (default 1) and STATICALLY preassigned to threads.
                    dynamic : loop iterations are divided into blocks of size chunk (default 1) and DYNAMICALLY scheduled amongs the threads so if one of the threads finish his work on a chunk it can immediatly start another one.
        - [Clauses] nowait
            Threads do not synchronize at the end of the paralell loop.
        - [Clauses] data scope (private, firstprivate, shared, reduction)
    
    - [Directives] sections
        #pragma omp sections [clause ...]
            {
                #pragma omp section
                    {
                    // execute A
                    }
                #pragma omp section
                    {
                    // execute B
                    }
            }
    
        Specifies the section(s) to be executed by the threads inside the team. Different sections can be exectuted by differente threads and it is possible that a thread can exectue more than a section.
        The Clauses are the same (private, reduction, nowait, etc.).
    
    - [Directives] single
        #pragma omp single [private | nowait | ...]
            {
            ...
            }
    
        Specifies that the following block can be exectued by one thread only, whichever it is so it can vary from a run to another.
    
    - [Directives] critical
        #pragma omp critical [ name ]
            {
            ...
            }
    
        Specifies a region that can be exectued by only one thread at a time. Behaves like a critical section, so if a thread is inside it and another one wants to enter it this one will wait untile the first one exits. 
            – Names act as global identifiers
            – Critical regions with the same name are treated as the same region (one
            global lock per name)
            – All critical sections which are unnamed, are treated as the same section
    
    
    - [Directives] atomic
        #pragma omp atomic
        <single instruction>
    
        Much faster than critical. Basically is the same concept but applied only to one instruction and specific ones (e.g. x++, x--, x+=expr, x*=expr)
    
    - [Directives] barrier
        #pragma omp barrier
    
        Barrier synchronizes all threads in the team. When a thread reaches it it must wait all threads that are left behind. When everybody have reached it they can go on.
    
    - [Clauses] Scoping
      - private(list) : in this list all declared variables are private to each thread. A new object of the same type is declared once for every thread and the original object references are replaced with reference to the new allocatedd object.
      Variables are uninitialized for each thread (so for example set to 0). 
      AFTER EXITING THE TEAM SECTION THE VARIABLE HAS THE SAME VALUE IT HAS BEEN INITIALIZED WITH.
        #pragma omp parallel private(i,a)
    
      - firstprivate(list) : this is the same of private with the difference that the variables inside the list are not uninitialized (e.g. 0) but are set to the global value before entering the private section.
      AFTER EXITING THE TEAM SECTION THE VARIABLE HAS THE SAME VALUE IT HAS BEEN INITIALIZED WITH.
      
      - shared (list) : this is shared among all threads in the team and it exists only in one memory location and all threads can read or write that address. Concurrency control is needed! 
      IF ARRAY IS STATIC (a[10]) it will be passed by copy. IF ARRAY IS DYNAMIC (vector<int> vec) it will be passed is pointer.
      #pragma omp parallel shared(a)
    
      - default (shared | none) : default scope of ALL VARIABLES IN THE LEXICAL EXTENT OF ANY PARALLEL REGION is either set to shared or none. 
    
      - reduction (operator: list) : with this you're passing a list of variables that will be taken as for 'private'. At the end of the parallel section all private copies of each thread will be joined togheter depending on the reduction 'operator' and the result will be written in the global shared variable using the reduction operator with the initial global value. E.g.:
          int i = 0;
        #pragma omp parallel num_threads(3) reduction(+ : i)
          {
            for (int k = 0; k < n; k++)
            {
              i++;
            } /*-- End of parallel for --*/
          }
        i = 0 => Result = 9 ; i = 2 => Result = 11
    
    
    
    
    
    