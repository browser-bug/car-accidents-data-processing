CSV Header:
DATE,TIME,BOROUGH,ZIP CODE,LATITUDE,LONGITUDE,LOCATION,ON STREET NAME,CROSS STREET NAME,OFF STREET NAME,NUMBER OF PERSONS INJURED,NUMBER OF PERSONS KILLED,NUMBER OF PEDESTRIANS INJURED,NUMBER OF PEDESTRIANS KILLED,NUMBER OF CYCLIST INJURED,NUMBER OF CYCLIST KILLED,NUMBER OF MOTORIST INJURED,NUMBER OF MOTORIST KILLED,CONTRIBUTING FACTOR VEHICLE 1,CONTRIBUTING FACTOR VEHICLE 2,CONTRIBUTING FACTOR VEHICLE 3,CONTRIBUTING FACTOR VEHICLE 4,CONTRIBUTING FACTOR VEHICLE 5,UNIQUE KEY,VEHICLE TYPE CODE 1,VEHICLE TYPE CODE 2,VEHICLE TYPE CODE 3,VEHICLE TYPE CODE 4,VEHICLE TYPE CODE 5

Todo:
  ☐ Capire perchè non CSVRow.cpp non vede le reference a CSVRow.h con code runner @started(20-02-26 23:35)
    Per il momento inseriamo tutto nell'header file a fini di testing. Probabilmente è un problema non risolvibile se non creando uno script. Essendo un lavoro un po' troppo tedioso credo che lascerò per il momento tutto così e in un secondo momento, quando il progetto sarà più robusto, creerò un makefile.

  ☐ Design statistic class @started(20-02-27 15:24)
    Ogni tupla del dataset ha una unique key che è possibile sfruttare per ricercare velocemente una determinata righa all'interno del file.
    
    ☐ Number of lethal accidents per week throughout the entire dataset. @started(20-02-27 15:27)
      In un anno ci sono 52 weeks. Per ogni settimana estraggo il numero di morti che sono legati agli incidenti stradali considerati come 'lethal accidents'.
      Conviene creare tante mappe quanti sono gli anni all'interno del dataset, quindi dato che va dal 2012 al 2017 si avranno 6 mappe.
      In ognuna di esse si utilizzerà una mappa del tipo {weekNumber, numDeaths}.

    ☐ Number of accidents and percentage of number of deaths per contributing factor in the dataset.@started(20-02-28 01:31)
      I.e., for each contributing factor, we want to know how many accidents were due to that contributing factor and what percentage of these accidents were also lethal.
      
      Il contributing factor può anche essere 'Unspecified' o 'Empty', in tal caso sarà ignorato durante il parsing.
      Ovviamente se una collisione ha uno o più contributing factor uguali lo andiamo a considerare solo una volta. Quindi guardiamo il CF_1, poi il CF_2, ecc.
      -> In ogni caso non c'è CF_5 senza CF_4, CF_4 senza CF_3 ecc. Quindi si può sempre partire da CF_1 e scorrere avanti.

      [numero di incidenti] basta semplicemente contare il numero di tuple all'interno del dataset corrispondenti a quel particolare contributing factor. 
      [percentuale di incidenti letali] durante il ciclo si aumenta un counter corrispondente al numero di incidenti letali e alla fine per calcolare la percentuale si farà -> perc = num_inc_letali / num_inc

      Sempre seguendo l'idea delle mappe conviene fare una mappa per ogni contributing factor ed associarvi una coppia std::pair contenente le due informazioni.

    ✔ Implement a function to return weekNumber based on date string @started(20-02-27 17:08) @done(20-02-27 18:20) @lasted(1h12m16s)

    ☐ Trovare una buona libreria per plottare l'istogramma dei dati raccolti. Verrebbe una cosa carina da affiancare alle statistiche temporali @started(20-02-27 18:21)

    ☐ Provare a testare se sia possibile integrare openMP oppure serva qualcosa di più "raw" @started(20-02-27 18:22)

OpenMP Notes:

      #pragma omp directive_name [clause, ...]
      {
      …
      }
    
    - I/O
      – OpenMP specifies nothing about parallel I/O
      – The programmer has to insure that I/O is conducted correctly within the context of a multi-threaded program
      
    - Compilation
      g++ hello.cc -fopenmp -o hello
      
    - Runtime 
      • If any thread terminates within a parallel region, all
      threads in the team terminate...
      • ...and the work done up until that point is undefined
      
    - API
      void omp_set_num_threads(int num_threads)
      
      int omp_get_num_threads(void)
    
      int omp_get_thread_num(void)
    
    - [Directives] parallel
      #pragma omp parallel [clause, clause, ... ]
      {
      // block
      }
      • When a thread reaches a parallel directive, it creates a team of threads and becomes the master of the team
      • The master is a member of that team and has thread number 0 within that team
        – Other threads have number 1...N-1
    
    - [Directives] for
        #pragma omp for [clause ...]
        for_loop
    
        This must be ecnlosed inside a parallel region in order for the directive to execute in parallel ( #pragma omp parallel for [clause ...] ). 
        This directive shares iterations of a for loop (which is indeed needed by the syntax). 
        - [Clauses] schedule
            schedule (type [,chunk])
            Describe how iterations of the loop are divided among the threads in the team. Default schedule is implementation dependent. 
                -> type
                    static : loop iterations are divided into blocks of size chunk (default 1) and STATICALLY preassigned to threads.
                    dynamic : loop iterations are divided into blocks of size chunk (default 1) and DYNAMICALLY scheduled amongs the threads so if one of the threads finish his work on a chunk it can immediatly start another one.
        - [Clauses] nowait
            Threads do not synchronize at the end of the paralell loop.
        - [Clauses] data scope (private, firstprivate, shared, reduction)
    
    - [Directives] sections
        #pragma omp sections [clause ...]
            {
                #pragma omp section
                    {
                    // execute A
                    }
                #pragma omp section
                    {
                    // execute B
                    }
            }
    
        Specifies the section(s) to be executed by the threads inside the team. Different sections can be exectuted by differente threads and it is possible that a thread can exectue more than a section.
        The Clauses are the same (private, reduction, nowait, etc.).
    
    - [Directives] single
        #pragma omp single [private | nowait | ...]
            {
            ...
            }
    
        Specifies that the following block can be exectued by one thread only, whichever it is so it can vary from a run to another.
    
    - [Directives] critical
        #pragma omp critical [ name ]
            {
            ...
            }
    
        Specifies a region that can be exectued by only one thread at a time. Behaves like a critical section, so if a thread is inside it and another one wants to enter it this one will wait untile the first one exits. 
            – Names act as global identifiers
            – Critical regions with the same name are treated as the same region (one
            global lock per name)
            – All critical sections which are unnamed, are treated as the same section
    
    
    - [Directives] atomic
        #pragma omp atomic
        <single instruction>
    
        Much faster than critical. Basically is the same concept but applied only to one instruction and specific ones (e.g. x++, x--, x+=expr, x*=expr)
    
    - [Directives] barrier
        #pragma omp barrier
    
        Barrier synchronizes all threads in the team. When a thread reaches it it must wait all threads that are left behind. When everybody have reached it they can go on.
    
    - [Clauses] Scoping
      - private(list) : in this list all declared variables are private to each thread. A new object of the same type is declared once for every thread and the original object references are replaced with reference to the new allocatedd object.
      Variables are uninitialized for each thread (so for example set to 0). 
      AFTER EXITING THE TEAM SECTION THE VARIABLE HAS THE SAME VALUE IT HAS BEEN INITIALIZED WITH.
        #pragma omp parallel private(i,a)
    
      - firstprivate(list) : this is the same of private with the difference that the variables inside the list are not uninitialized (e.g. 0) but are set to the global value before entering the private section.
      AFTER EXITING THE TEAM SECTION THE VARIABLE HAS THE SAME VALUE IT HAS BEEN INITIALIZED WITH.
      
      - shared (list) : this is shared among all threads in the team and it exists only in one memory location and all threads can read or write that address. Concurrency control is needed! 
      IF ARRAY IS STATIC (a[10]) it will be passed by copy. IF ARRAY IS DYNAMIC (vector<int> vec) it will be passed is pointer.
      #pragma omp parallel shared(a)
    
      - default (shared | none) : default scope of ALL VARIABLES IN THE LEXICAL EXTENT OF ANY PARALLEL REGION is either set to shared or none. 
    
      - reduction (operator: list) : with this you're passing a list of variables that will be taken as for 'private'. At the end of the parallel section all private copies of each thread will be joined togheter depending on the reduction 'operator' and the result will be written in the global shared variable using the reduction operator with the initial global value. E.g.:
          int i = 0;
        #pragma omp parallel num_threads(3) reduction(+ : i)
          {
            for (int k = 0; k < n; k++)
            {
              i++;
            } /*-- End of parallel for --*/
          }
        i = 0 => Result = 9 ; i = 2 => Result = 11
    
    
    
    
    
    